import torch
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.transforms import functional as F
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# üîÅ –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ —Ç–≤–æ—è –º–∞–ø–∞ id -> label
id2label = {
    1: 'pavement',
    2: 'ramp',
    3: 'step',
    4: 'stair',
    5: 'grab_bar'
}

# ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def load_model(path, num_classes, device="cpu"):
    # 1) –±–µ—Ä–µ–º–æ –ø—Ä–µ—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –Ω–∞ COCO –º–æ–¥–µ–ª—å
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights, pretrained=True)
    # 2) –º—ñ–Ω—è—î–º–æ –≥–æ–ª–æ–≤—É –ø—ñ–¥ —Å–≤—ñ–π num_classes
    in_feats = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)
    # 3) –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–≤–æ—ó –≤–∞–≥–∏
    # checkpoint = torch.load(path, map_location=device)
    # # —è–∫—â–æ —Ç–∏ –∑–±–µ—Ä—ñ–≥–∞–≤ –ø–æ–≤–Ω–∏–π state_dict:
    # model.load_state_dict(checkpoint)
    # –∞–±–æ, —è–∫—â–æ —Ç–∏ –∑–±–µ—Ä—ñ–≥–∞–≤ —á–µ–∫–ø–æ—ñ–Ω—Ç —ñ–∑ –∫–ª—é—á–µ–º 'model_state_dict':
    # model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device).eval()
    return model



def predict_labels_with_scores(model, image_path, id2label, score_threshold=0.0, device="cpu"):
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î dict: {label_name: max_score} –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å—ñ–≤ —ñ–∑ id2label.
    –Ø–∫—â–æ –∫–ª–∞—Å –Ω–µ –±—É–ª–æ –≤–∏—è–≤–ª–µ–Ω–æ –∑ score >= score_threshold, –π–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è –±—É–¥–µ 0.0.
    """
    # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –Ω—É–ª—è–º–∏ –¥–ª—è –≤—Å—ñ—Ö –ª–µ–π–±–ª—ñ–≤
    result = {name: 0.0 for name in id2label.values()}

    # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –π –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    image = Image.open(image_path).convert("RGB")
    image_tensor = F.to_tensor(image).to(device)

    # 3. –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å
    model.to(device).eval()
    with torch.no_grad():
        output = model([image_tensor])[0]

    # 4. –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è—Ö
    labels = output['labels'].cpu()
    scores = output['scores'].cpu()

    for lbl, sc in zip(labels, scores):
        sc_val = sc.item()
        if sc_val < score_threshold:
            continue
        name = id2label.get(lbl.item(), str(lbl.item()))
        # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ª–∏—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π score –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ label
        if sc_val > result[name]:
            result[name] = sc_val

    return result


def analyze_image(model, image_path, score_threshold=0.2):
    image = Image.open(image_path).convert("RGB")
    image_tensor = F.to_tensor(image)

    with torch.no_grad():
        output = model([image_tensor])[0]

    # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ score
    boxes = output['boxes']
    labels = output['labels']
    scores = output['scores']

    fig, ax = plt.subplots(1)
    ax.imshow(image)

    for box, label, score in zip(boxes, labels, scores):
        if score >= score_threshold:
            xmin, ymin, xmax, ymax = box
            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                     linewidth=2, edgecolor='lime', facecolor='none')
            ax.add_patch(rect)
            label_text = f"{id2label.get(label.item(), label.item())} ({score:.2f})"
            ax.text(xmin, ymin - 5, label_text, color='lime', fontsize=9, backgroundcolor='black')

    plt.axis('off')
    plt.show()


id2label = {1:'pavement', 2:'ramp', 3:'step', 4:'stair', 5:'grab_bar'}

# üîÅ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
model = load_model("Accessibility Barriers/code/checkpoint4.pth", num_classes=6)  # 5 –∫–ª–∞—Å—ñ–≤ + background
print(model.transform)

analyze_image(model, "Accessibility Barriers/code/images.jpg")
labels_scores = predict_labels_with_scores(model, "Accessibility Barriers/code/images.jpg", id2label, score_threshold=0.3)


print(labels_scores)

import os

print(os.getcwd())  # –î–æ–¥–∞–Ω–æ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –ø–æ—Ç–æ—á–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó